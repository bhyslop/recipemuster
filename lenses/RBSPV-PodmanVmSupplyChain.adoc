= Podman VM Supply Chain

== Motivation

Podman is an excellent open-source tool, but its VM image distribution model creates a serious reproducibility hazard. After months of stable development using a particular Podman version, deleting and redownloading the VM with the same semantic version silently produced a fundamentally different image. The entire SENTRY/BOTTLE network architecture — which had been working reliably — broke. Virtual networking inside containers stopped functioning, and the failure mode gave no indication that the underlying VM image had changed.

Systematic investigation (documented in Study/study-net-namespace-permutes/) revealed that the new VM image carried Podman 5.3+ security hardening that prohibited `setns()` on externally-created network namespaces. This broke Recipe Bottle's core isolation mechanism: containers could no longer join pre-configured network namespaces, regardless of privileges or creation method. The definitive post-mortem in Study/study-net-namespace-permutes/claudeopus4-netns-fail-memo.md confirmed this was not a configuration issue but a fundamental change in the VM's capabilities. The earlier networking journey is recorded in podman-gateway-proposal.md and the iptables architecture failures in Study/study-shared-pod-userid/ and Study/study-shared-pod-ip-forward/.

The root cause is quay.io's build and retention policy. The upstream registry rebuilds VM images approximately every three hours and deletes older versions. Version tags are not immutable within a Podman semantic version — downloading the same tag at different times produces different disk images with different capabilities. There is no way to recover a previous image once it has been replaced.

This motivated bringing VM images under user control by mirroring them into the project's own Depot artifact registry. By maintaining stable, versioned VM images in a controlled repository, Recipe Bottle can ensure reproducible environments and prevent silent regressions from upstream image churn.

== Constraints

Several technical and architectural constraints shape the VM supply chain design.

VM images are OCI artifacts rather than standard container images. They contain compressed root filesystem tarballs for WSL environments or CoreOS disk images for standard virtualization platforms. These artifacts use non-standard media types, rely on custom annotations such as disktype to indicate their intended use, and carry opaque blobs rather than composable filesystem layers. While they are platform-indexed in the OCI sense, they are not platform-runnable as containers would be. Because of these differences, standard `podman pull` and `docker pull` cannot correctly retrieve and reassemble VM artifacts — they operate at the container-runtime abstraction level, which assumes runnable images with standard layer structure. Tools like crane (manifest and blob-level registry operations), skopeo (cross-registry copying with format awareness), and oras (OCI Registry As Storage, designed for non-container artifacts) work below that abstraction at the registry API and manifest level, which is why they can handle VM artifacts correctly.

The bootstrap process faces a chicken-and-egg problem. The mirroring workflow requires crane, oras, or skopeo, but Recipe Bottle does not want to obligate users to install and maintain these tools on their host systems. The solution is to use an ignite VM, initialized with the latest uncontrolled upstream image, as a disposable tooling environment. Crane and jq are installed inside the ignite VM, and all registry operations execute there. The ignite VM is discarded after completing the mirroring operation.

Establishing provenance for VM images is difficult. When podman machine init runs, it prints a SHA digest during initialization that can be captured and compared against expected values. However, this information is only available at initialization time. After a VM has been created, there is no reliable way to verify which specific image was used to initialize it. This limitation makes post-hoc auditing challenging and emphasizes the importance of capturing provenance information during the init operation.

The technical constraint that VM images are locked to a specific Podman semantic version but not immutable within that version creates ongoing maintenance challenges. Two distinct image families exist: machine-os-wsl for WSL environments and machine-os for standard CoreOS-based systems. These families have different build processes, different artifact structures, and different platform variants, requiring parallel handling in the mirroring workflow.

Storage cost considerations favor a conservative approach. Users do not frequently update their Podman VM installations, so maintaining a few full disk images in the Depot is practical. The storage expense is acceptable given the stability and reproducibility benefits.

== Architecture

The VM supply chain architecture centers on a two-machine pattern. The ignite machine serves as a temporary, disposable tooling environment. It is initialized with whatever upstream image Podman naturally fetches and is used only to run crane, podman, and other OCI utilities needed for manifest introspection and blob manipulation. The deploy machine, by contrast, is the persistent, user-controlled VM initialized from carefully selected and cached images stored in the Depot.

The overall lifecycle follows a sequence of operations: check, mirror, fetch, init, and start or stop. The check operation discovers upstream updates by comparing digests between the current registry state and what is available upstream. The mirror operation queries upstream manifests for both WSL and standard image families, downloads individual disk blobs, repackages them as container images, and pushes them to the registry with platform-specific tags. The fetch operation pulls the platform-specific container image from the registry and extracts the disk image to a local cache directory. The init operation initializes the deploy VM from the cached disk image and writes a brand file containing full provenance information. Finally, start and stop operations manage the deploy VM's runtime state. A nuke operation destroys all VMs, both ignite and deploy, along with the local cache, enabling a complete reset of the environment.

This separation between ignite and deploy machines provides a clear boundary between uncontrolled tooling infrastructure and the production-quality VM environment that hosts containers.

== Operations

=== Check (unimplemented)

Discovers when new VM images are available upstream by comparing digest values between the current registry state and what quay.io currently offers. This operation was specified in earlier documentation but has never been implemented in bash.

=== Mirror

The most complex operation. Steps:

. Bootstrap the ignite VM with crane and jq installed
. Query upstream manifests for both machine-os-wsl and machine-os image families at the specified Podman version
. Parse manifests to identify all available platform variants (architecture + disk type)
. For each platform variant, download the individual disk image blob from upstream using `crane blob`
. Repackage each blob as a container image: a simple Dockerfile copies the disk blob into a scratch-based container, with an identity file for traceability
. Build each platform-specific image locally within the ignite VM
. Push to the registry with tags encoding the Podman version, a timestamp-based identity, and the platform specification

The result is a set of individually addressable container images, each containing a specific disk image for a specific platform variant.

=== Fetch

Pulls a platform-specific container image from the registry to the local workstation:

. Start the ignite VM to access the podman image cache
. Authenticate to the registry
. Pull the platform-specific container image matching the user's station configuration
. Extract the disk image tarball from the container
. Write it to the local cache directory (always overwrites existing)

The cached file serves as the source for subsequent VM initialization.

=== Init

Creates the deploy VM from the cached disk image:

. Validate that the requested platform matches one of the available manifest platforms
. Run `podman machine init` with the rootful flag and the path to the cached image
. Start the VM temporarily to write a brand file containing the full provenance chain: Podman version, VM image origin, fully qualified image name, digest, and identity timestamp
. Stop the VM, leaving it ready for normal use

The brand file provides an auditable record of exactly which image was used to create the VM.

=== Start / Stop

Straightforward lifecycle commands. Start brings the deploy VM online for container operations. Stop gracefully shuts it down. These do not affect the ignite VM.

=== Nuke

Complete environment reset:

. Stop and remove all containers
. Destroy both the ignite and deploy VMs
. Delete the VM cache directory

Requires explicit confirmation. Used when switching Podman versions, recovering from corrupted VM states, or performing clean test runs.

== Configuration

The VM supply chain relies on two categories of configuration variables. Regime configuration variables are stored in the repository and apply to all users working within the Recipe Bottle project. Station configuration variables are stored on each workstation and reflect local machine characteristics.

The regime configuration includes several key variables. RBRR_IGNITE_MACHINE_NAME specifies the name for the temporary tooling VM. RBRR_DEPLOY_MACHINE_NAME specifies the name for the persistent production VM. RBRR_CRANE_TAR_GZ provides the URL for downloading the crane binary into the ignite VM. RBRR_CHOSEN_PODMAN_VERSION identifies which Podman semantic version to mirror and use. RBRR_CHOSEN_VMIMAGE_ORIGIN records the upstream source registry. RBRR_CHOSEN_IDENTITY is a timestamp-based identifier assigned during the mirror operation to distinguish different builds of the same Podman version. RBRR_MANIFEST_PLATFORMS lists the platform variants that must be available in the upstream manifest and should be mirrored to the registry. RBRR_GITHUB_PAT_ENV provides the path to the credentials file used for registry authentication during mirror and fetch operations.

The station configuration includes variables that vary by workstation. RBRS_PODMAN_ROOT_DIR points to the base directory where Podman stores machine data. RBRS_VMIMAGE_CACHE_DIR specifies where fetched disk images should be cached locally. RBRS_VM_PLATFORM identifies which platform variant the local machine should use, such as mow_amd64_wsl or mos_arm64_base.

These variables are currently defined in the broader RBRR configuration but are expected to be extracted into a dedicated VM regime in future refactoring. This separation will clarify the distinction between Recipe Bottle's application-level configuration and the underlying VM infrastructure configuration.

An important transitional note concerns GHCR-era variables. The original implementation targeted GitHub Container Registry and used variables like RBRR_REGISTRY_OWNER, RBRR_REGISTRY_NAME, and RBRR_GITHUB_PAT_ENV to manage authentication and registry addressing. These variables are being deleted as part of ongoing work. When Google Artifact Registry support is implemented, new properly scoped variables will be minted following the project's prefix naming discipline.

== Implementation Status

The current implementation exists in Tools/rbw/rbv_PodmanVM.sh and targets GitHub Container Registry. This bash module implements the mirror, fetch, init, start, stop, and nuke operations as described above. However, the check operation, which would discover upstream updates by comparing digests, has no bash implementation and exists only as a concept.

The codebase reflects an evolution well beyond the original specifications. Earlier specification documents RBSVC and RBSVM described a simpler design in which crane would copy entire images directly from upstream to the registry. This design envisioned variables like rbrr_chosen_vmimage_fqin and rbrr_chosen_vmimage_sha that would reference complete images. These variables were never implemented in code and remain spec-only ghosts.

The actual bash implementation evolved to a more sophisticated approach involving blob-level extraction, repackaging of disk images as container images, support for both WSL and standard image families, iteration across multiple platform variants, and identity stamping for build traceability. This evolution reflects lessons learned during implementation about the structure of OCI VM artifacts and the need for fine-grained control over individual disk images.

The vme.extractor.sh script was an earlier standalone tool for manifest introspection. It has been superseded by the zrbv_process_image_type function in the modern bash module, which integrates manifest parsing directly into the mirroring workflow.

The file rbp.podman.mk contains historical Makefile prototypes from the GHCR era. These Makefile targets served as an initial exploration of the VM mirroring workflow but have been fully superseded by the rbv_PodmanVM.sh module, which provides a more robust and maintainable implementation.

The implementation has never been ported from GitHub Container Registry to Google Artifact Registry. This migration remains necessary for production use within the Recipe Bottle architecture, which standardizes on Google Cloud infrastructure for its depot and service account management.

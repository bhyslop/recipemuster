= Recipe Bottle Makefile Service
:toc:
:toc-title: Table of Contents
:sectnums:
:icons: font
:index:

// Begin Mapping Section
// tag::mapping-section[]

// Category declarations:
// at_ prefix: Architectural Term (core system components)
// st_ prefix: Support Term (infrastructure and technical concepts)
// ua_ prefix: User Action (operations performed by users)
// ops_ prefix: Sentry startup sequence aspects
// opbs_ prefix: Sessile Bottle startup sequence aspects
// opbr_ prefix: Agile Bottle startup sequence aspects
// rbb_ prefix: Base configuration aspects
// rbrn_ prefix: Nameplate configuration aspects
// cmk_ prefix: Console Makefile elements - standard build system patterns

// Variant patterns:
//   _s: plural form
//   _p: possessive form
//   _ed: past tense (verbs)
//   _ing: progressive form (verbs)

// Domain declaration: term_<canonical_term>

:at_bottle_image:            <<bottle_image,Bottle Image>>
:at_bottle_service:          <<bottle_service,Bottle Service>>
:at_bottle_service_s:        <<bottle_service,Bottle Services>>
:at_sessile_service:         <<sessile_service,Sessile Bottle Service>>
:at_agile_service:           <<agile_service,Agile Bottle Service>>
:at_workstation:             <<workstation,Workstation>>
:at_workstation_p:           <<workstation,Workstation's>>
:at_enclave_network:         <<enclave_network,Enclave Network>>
:at_enclave_network_s:       <<enclave_network,Enclave Network>>
:at_transit_network:         <<transit_network,Transit Network>>
:at_moniker:                 <<moniker,Moniker>>
:at_nameplate:               <<nameplate,Nameplate>>
:at_nameplate_s:             <<nameplate,Nameplates>>
:at_transit_network_name:    <<transit_network_name,Transit Network Name>>
:at_enclave_namespace_name:  <<enclave_namespace_name,Enclave Namespace Name>>
:at_enclave_network_name:    <<enclave_network_name,Enclave Network Name>>
:at_sentry_image:            <<sentry_image,Sentry Image>>
:at_sentry_container:        <<sentry_container,Sentry Container>>
:at_sentry_container_s:      <<sentry_container,Sentry Containers>>
:at_sentry_container_p:      <<sentry_container,Sentry Container's>>
:at_censer_container:        <<censer_container,Censer Container>>
:at_censer_container_s:      <<censer_container,Censer Containers>>
:at_bottle_container:        <<bottle_container,Bottle Container>>
:at_bottle_container_s:      <<bottle_container,Bottle Containers>>
:at_bottle_container_p:      <<bottle_container,Bottle Container's>>
:at_bottle_container_name:   <<bottle_container_name,Bottle Container Name>>
:at_sentry_container_name:   <<sentry_container_name,Sentry Container Name>>
:at_rbm_console:             <<rbm_console,RBM Console Makefile>>
:at_rbm_config_makefile:     <<rbm_config_makefile,RBM Config Makefile>>
:at_rbm_system:              <<rbm_system,RBM System>>
:at_rbm_system_p:            <<rbm_system,RBM System's>>
:at_rbm_secret:              <<rbm_secret,RBM Secret>>
:at_rbm_secret_s:            <<rbm_secret,RBM Secrets>>
:at_rbm_repo:                <<rbm_repo,RBM Repository>>
:at_user_repo:               <<user_repo,User Repository>>
:at_container_registry:      <<container_registry,Remote Container Registry>>
:at_startup_script:          <<startup_script,Startup Script>>
:at_startup_script_s:        <<startup_script,Startup Scripts>>
:rbtr_consumer:              <<consumer,Consumer Role>>
:rbtr_consumer_s:            <<consumer,Consumer Roles>>
:rbtr_consumer_p:            <<consumer,Consumer Role's>>
:at_ebpf_program:            <<at_ebpf_program,eBPF Program>>
:at_stash_machine:           <<stash_machine,Stash Machine>>
:at_operational_machine:     <<operational_machine,Operational Machine>>

:st_container:               <<container,Container>>
:st_container_s:             <<container,Containers>>
:st_subnet:                  <<subnet,Subnet>>
:st_gateway:                 <<gateway,Gateway>>
:st_dockerfile:              <<dockerfile,Dockerfile>>
:st_published_image:         <<published_image,Published Image>>
:st_published_image_s:       <<published_image,Published Images>>
:st_image:                   <<image,Container Image>>
:st_image_s:                 <<image,Container Images>>
:st_image_store:             <<image_store,Local Image Store>>
:st_port_map:                <<port_map,Port Map>>
:st_port_map_s:              <<port_map,Port Maps>>
:st_volume_mount:            <<volume_mount,Volume Mount>>
:st_volume_mount_s:          <<volume_mount,Volume Mounts>>

:opss_sentry_start:          <<opss_sentry_start,Sentry Start Rule>>
:opbs_bottle_start:          <<opbs_bottle_start,Sessile Bottle Start Rule>>
:opbr_bottle_run:            <<opbr_bottle_run,Agile Bottle Run Rule>>

:mkr_network_create:         <<mkr_network_create,Network Creation Sequence>>
:mkr_sentry_run:             <<mkr_sentry_run,Sentry Launch Sequence>>
:mkr_network_connect:        <<mkr_network_connect,Network Connection Sequence>>
:mkr_bottle_cleanup:         <<mkr_bottle_cleanup,Bottle Cleanup Sequence>>
:mkr_bottle_launch:          <<mkr_bottle_launch,Bottle Launch Sequence>>
:mkr_bottle_create:          <<mkr_bottle_create,Bottle Create Sequence>>
:mkr_command_exec:           <<mkr_command_exec,Command Execution Sequence>>

:ops_rbv_check:              <<ops_rbv_check,RBV Check Operation>>
:ops_rbv_mirror:             <<ops_rbv_mirror,RBV Mirror Operation>>

:mkc_interface_check:        <<mkc_interface_check,Interface Check Clause>>

:scr_security_config:        <<scr_security_config,Security Configuration Stage>>
:scr_iptables_init:          <<scr_iptables_init,IPTables Initialization Sequence>>
:scr_access_setup:           <<scr_access_setup,Access Control Setup Sequence>>
:scr_port_setup:             <<scr_port_setup,Port Configuration Sequence>>
:scr_dns_step:               <<scr_dns_step,DNS Configuration Sequence>>

:cfg_base_regime:           <<base_regime,Base Config Regime>>
:cfg_base_regime_s:         <<base_regime,Base Config Regimes>>
:cfg_station_regime:        <<station_regime,Station Config Regime>>
:cfg_station_regime_s:      <<station_regime,Station Config Regimes>>
:cfg_nameplate_regime:      <<nameplate_regime,Nameplate Config Regime>>
:cfg_nameplate_regime_s:    <<nameplate_regime,Nameplate Config Regimes>>
:cfg_regime_variable:       <<regime_variable,Config Regime Variable>>
:cfg_regime_variable_s:     <<regime_variable,Config Regime Variables>>

:cmk_discipline:             <<cmk_discipline,Console Makefile Discipline>>
:cmk_external_rule:          <<cmk_external_rule,External Rule>>
:cmk_external_rule_s:        <<cmk_external_rule,External Rules>>
:cmk_internal_rule:          <<cmk_internal_rule,Internal Rule>>
:cmk_internal_rule_s:        <<cmk_internal_rule,Internal Rules>>
:cmk_makefile:               <<cmk_makefile,Console Style Makefile>>
:cmk_clause:                 <<cmk_clause,Makefile Clause>>
:cmk_recipe_line:            <<cmk_recipe_line,Makefile Recipe Line>>
:cmk_recipe_line_s:          <<cmk_recipe_line,Makefile Recipe Lines>>
:cmk_probe_line:             <<cmk_probe_line,Makefile Probe Line>>
:cmk_probe_line_s:           <<cmk_probe_line,Makefile Probe Lines>>
:cmk_script:                 <<cmk_script,Console Support Script>>
:cmk_script_s:               <<cmk_script,Console Support Scripts>>
:cmk_script_line:            <<cmk_script_line,Console Script Line>>
:cmk_script_line_s:          <<cmk_script_line,Console Script Lines>>

:rbrr_dns_server:            <<rbrr_dns_server,RBRR_DNS_SERVER>>
:rbrr_nameplate_path:        <<rbrr_nameplate_path,RBRR_NAMEPLATE_PATH>>
:rbrr_registry_server:       <<rbrr_registry_server,RBRR_REGISTRY_SERVER>>
:rbrr_registry_owner:        <<rbrr_registry_owner,RBRR_REGISTRY_OWNER>>
:rbrr_registry_name:         <<rbrr_registry_name,RBRR_REGISTRY_NAME>>
:rbrr_github_pat_env:        <<rbrr_github_pat_env,RBRR_GITHUB_PAT_ENV>>
:rbrr_build_architectures:   <<rbrr_build_architectures,RBRR_BUILD_ARCHITECTURES>>
:rbrr_history_dir:           <<rbrr_history_dir,RBRR_HISTORY_DIR>>
:rbrr_chosen_podman_version: <<rbrr_chosen_podman_version,RBRR_CHOSEN_PODMAN_VERSION>>
:rbrr_chosen_vmimage_origin: <<rbrr_chosen_vmimage_origin,RBRR_CHOSEN_VMIMAGE_ORIGIN>>
:rbrr_chosen_vmimage_fqin:   <<rbrr_chosen_vmimage_fqin,RBRR_CHOSEN_VMIMAGE_FQIN>>
:rbrr_chosen_vmimage_sha:    <<rbrr_chosen_vmimage_sha,RBRR_CHOSEN_VMIMAGE_SHA>>
:rbrr_chosen_identity:       <<rbrr_chosen_identity,RBRR_CHOSEN_IDENTITY>>
:rbrr_stash_machine:         <<rbrr_stash_machine,RBRR_STASH_MACHINE>>
:rbrr_operational_machine:   <<rbrr_operational_machine,RBRR_OPERATIONAL_MACHINE>>
:rbrr_crane_tar_gz:          <<rbrr_crane_tar_gz,RBRR_CRANE_TAR_GZ>>

:rbrn_moniker:                    <<rbrn_moniker,RBRN_MONIKER>>
:rbrn_description:                <<rbrn_description,RBRN_DESCRIPTION>>
:rbrn_sentry_moniker:             <<rbrn_sentry_moniker,RBRN_SENTRY_MONIKER>>
:rbrn_bottle_moniker:             <<rbrn_bottle_moniker,RBRN_BOTTLE_MONIKER>>
:rbrn_sentry_image_tag:           <<rbrn_sentry_image_tag,RBRN_SENTRY_IMAGE_TAG>>
:rbrn_bottle_image_tag:           <<rbrn_bottle_image_tag,RBRN_BOTTLE_IMAGE_TAG>>
:rbrn_entry_mode:                 <<rbrn_entry_mode,RBRN_ENTRY_MODE>>
:rbrn_entry_port_workstation:     <<rbrn_entry_port_workstation,RBRN_ENTRY_PORT_WORKSTATION>>
:rbrn_entry_port_enclave:         <<rbrn_entry_port_enclave,RBRN_ENTRY_PORT_ENCLAVE>>
:rbrn_uplink_dns_mode:            <<rbrn_uplink_dns_mode,RBRN_UPLINK_DNS_MODE>>
:rbrn_uplink_access_mode:         <<rbrn_uplink_access_mode,RBRN_UPLINK_ACCESS_MODE>>
:rbrn_uplink_allowed_cidrs:       <<rbrn_uplink_allowed_cidrs,RBRN_UPLINK_ALLOWED_CIDRS>>
:rbrn_uplink_allowed_domains:     <<rbrn_uplink_allowed_domains,RBRN_UPLINK_ALLOWED_DOMAINS>>
:rbrn_enclave_netmask:            <<rbrn_enclave_netmask,RBRN_ENCLAVE_NETMASK>>
:rbrn_enclave_base_ip:            <<rbrn_enclave_base_ip,RBRN_ENCLAVE_BASE_IP>>
:rbrn_enclave_sentry_ip:          <<rbrn_enclave_sentry_ip,RBRN_ENCLAVE_SENTRY_IP>>
:rbrn_volume_mounts:              <<rbrn_volume_mounts,RBRN_VOLUME_MOUNTS>>

// end::mapping-section[]

[abstract]
.Abstract
The Recipe Bottle Makefile Service enables safe and effective use of foreign
{st_container_s}
at the
{rbtr_consumer}
{at_workstation}.

== System Overview 

=== Core Trust Challenge

Modern development increasingly requires running foreign code with access to local work.

{st_container}-
based development tools processing source code, local AI/ML tools analyzing proprietary data, and third-party services needing controlled network access create fundamental security challenges.

This creates a fundamental trust dilemma:

* {st_container_s}
provide powerful tool isolation
* But they need controlled access
to do useful work  
* Standard
{st_container}
networking
grants unfettered outbound access or else network services completely unavailable.

=== The Bottle Pattern

{at_rbm_system}
solves this by creating two simple, robust boundaries around a certain
{st_container}
type here referred to as a
{at_bottle_container}:

. File Access Boundary
* {at_bottle_container}
sees only explicitly controlled set of
{st_volume_mount_s}
* No other access
to
{at_workstation}
filesystem

. Network Boundary  
* {at_bottle_container}
has no direct network access
* All traffic
flows through dedicated
{at_sentry_container},
a
{st_container}
configured with very tight network access and visibility controls.
* Only declared
{st_port_map_s}
and destinations
allowed
* Full logging
of allowed and denied traffic

Each
{at_bottle_service}
maintains these boundaries through a three-container architecture:

* One
{at_sentry_container}
enforcing network policy on dual networks (bridge and enclave)
* One
{at_censer_container}
establishing and configuring the network namespace
* One
{at_bottle_container}
running the actual service, sharing the censer's network namespace
* Clear configuration
in a single
{at_nameplate}
file

The
{at_censer_container}
is a privileged container that establishes the network namespace, configures routing to ensure all traffic flows through the
{at_sentry_container},
then shares this pre-configured namespace with the
{at_bottle_container}
via `--net=container:censer`.
This ensures security policies are enforced from the first packet, and the
{at_bottle_container}
experiences only a functional path to its
{at_sentry_container}
gateway

=== Security Properties

The
{at_rbm_system}
maintains following essential guarantees:

. File Access Control
* {at_bottle_container_s}
can access only explicitly configured
{st_volume_mount_s}
* No access to broader system resources

. Network Isolation 
* All traffic flows through dedicated
{at_sentry_container}
* Each permitted traffic flow maintains exactly one controlled pathway
* Precise ingress, forwarding, and NAT rules enforce strict boundaries
* Only explicitly configured
{st_port_map_s}
allow traffic between
{at_transit_network} and {at_enclave_network}
* {at_bottle_container_s}
have no direct network access; all connectivity flows through
{at_sentry_container_s}
* Comprehensive traffic logging

. Service Separation
* Each
{at_bottle_service}
fully isolated
* No cross-service communication
* Independent security policies
* Clean state separation

// TODO fixup this language realized missing during pod namespace try
. Service Access Control
* External access to services only permitted when {rbrn_entry_mode} is enabled
* No inter-container communication allowed within pods
* All service access must originate from outside the pod
* Entry traffic flows: external → pod →
{at_sentry_container} → {at_bottle_container}

These properties hold across all operations and failure modes.

== {opss_sentry_start}

The
{at_rbm_console}
defines the
{opss_sentry_start},
a
{cmk_external_rule}
which is the precisely ordered sequence of
{cmk_recipe_line_s}
needed to start a
{at_sentry_container}
that enforces network policy for a new
{at_bottle_service}.

=== {mkr_network_create}

This sequence of
{cmk_recipe_line_s}
within the
{opss_sentry_start}
establishes required networks for the
{at_bottle_service}:

. Remove Existing
{at_transit_network}
* Remove the
{at_transit_network}
named
{at_transit_network_name},
ignoring errors
* This ensures a clean state before creating the new transit network

. Create New
{at_transit_network}
* Use standard Podman network primitives to create a bridge network named
{at_transit_network_name}
* The IP address range or any other attributes remain consistent with the
{rbrn_moniker}-uplink
definition
* IPv6 remains disabled, and external connectivity is enabled

. Remove Existing
{at_enclave_network}
* Remove the
{at_enclave_network}
named
{at_enclave_network_name},
ignoring errors
* This ensures a clean state before creating the new enclave network

. Create New
{at_enclave_network}
* Use standard Podman network primitives to create an isolated bridge network named
{at_enclave_network_name}
* Configure the network with subnet
{rbrn_enclave_base_ip}/{rbrn_enclave_netmask}
* Disable external connectivity to ensure isolation
* Allow netavark to manage IP address assignment for containers on this network

After this sequence completes, both networks exist in standard Podman configuration. The
{at_sentry_container}
will connect to both networks, while the
{at_bottle_container}
will connect only to the
{at_enclave_network}.
Before the
{at_bottle_container}
starts, an
{at_ebpf_program}
will be attached to its veth interface to ensure all traffic routes through the
{at_sentry_container}.

=== {mkr_network_connect}

This sequence of
{cmk_recipe_line_s}
within the
{opss_sentry_start}
establishes the secondary network interface connected to
{at_enclave_network}:

. Connect Container to Enclave Network
* Attach the
{at_sentry_container}
to the
{at_enclave_network}
using standard Podman network connect
* Verify successful network attachment
* Validate interface naming sequence - eth0 must be the transit network interface
* The enclave network interface must appear as eth1

. Verify Network Interface Configuration
* Execute within sentry container context
* {mkc_interface_check}
with interface `eth1` and timeout_s `5`
** Exit on first success
** Fail if timeout reached

. Configure Sentry IP Address
* Remove the auto-assigned IP address from eth1
* Assign the permanent
{rbrn_enclave_sentry_ip}/{rbrn_enclave_netmask}
address to eth1
* This address will serve as the gateway for the
{at_bottle_container}
after eBPF frame rewriting

After this sequence completes, the
{at_sentry_container}
has connectivity on both networks with the correct IP configuration, ready to receive traffic from the
{at_bottle_container}
once the
{at_ebpf_program}
is attached.

=== {scr_security_config}

One unified script executes these
{cmk_script_line_s}
in sequence, halting early if any
{cmk_script_line}
has a nonzero unix status:

1. {scr_iptables_init} 
2. {scr_port_setup}
3. {scr_access_setup}
4. {scr_dns_step}

==== Phase 1: {scr_iptables_init} 

Establishes foundational security state:

. Base Chain Configuration:
* Flush all rules from filter and nat tables
* Set policy DROP on all base chains (INPUT, FORWARD, OUTPUT)
* Accept loopback interface traffic unconditionally via first rule in INPUT chain

. State Tracking Setup:
* Insert RELATED,ESTABLISHED tracking as second rule in chains:
** INPUT chain with "-m state --state RELATED,ESTABLISHED -j ACCEPT"
** FORWARD chain with same state match
** OUTPUT chain with same state match

. RBM Chain Creation:
* Create in filter table with default policy DROP:
** RBM-INGRESS for input control
** RBM-EGRESS for output control
** RBM-FORWARD for forwarding control
* Insert jumps as third rule in respective base chains:
** INPUT jumps to RBM-INGRESS
** OUTPUT jumps to RBM-EGRESS
** FORWARD jumps to RBM-FORWARD

==== Phase 2: {scr_port_setup}

If
{rbrn_entry_mode}
is disabled, this sequence is skipped.
If
{rbrn_entry_mode}
is enabled, this sequence configures service port mapping:

. Socat Proxy Configuration:
* Launch socat proxy process in background:
** Listen on TCP port
{rbrn_entry_port_workstation}
** Forward connections to
{at_bottle_container}
IP (
{rbrn_enclave_bottle_ip}
) on port
{rbrn_entry_port_enclave}
** Enable connection forking and address reuse
** Log output to `/var/log/socat-proxy.log`
* Verify socat process started successfully

. Filter Configuration:
* Add rule to RBM-INGRESS accepting incoming traffic from bridge network:
** Interface eth0
** Protocol TCP
** Destination port
{rbrn_entry_port_workstation}
* Add rule to RBM-EGRESS accepting outbound connections to bottle:
** Interface eth1
** Protocol TCP
** Destination
{rbrn_enclave_bottle_ip}
** Destination port
{rbrn_entry_port_enclave}
* Add rule to RBM-INGRESS accepting return traffic from bottle:
** Interface eth1
** Protocol TCP
** Source
{rbrn_enclave_bottle_ip}
** Source port
{rbrn_entry_port_enclave}

==== Phase 3: {scr_access_setup}

If
{rbrn_uplink_access_mode}
is disabled, this sequence:

* Verifies DROP policy exists for all filter table chains
* Removes NAT rules except those marked "RBM-PORT-FORWARD"
* Adds explicit DROP rules for non-port traffic:
** RBM-EGRESS: DROP all eth0 output
** RBM-FORWARD: DROP all eth1 forward

If
{rbrn_uplink_access_mode}
is global or allowlist, this sequence:

. Network Interface Configuration:
* Enable IPv4 forwarding
* Disable IPv6
* Set rp_filter to strict mode (1) on all interfaces
* Set route_localnet on eth0

. NAT Configuration:
* Add POSTROUTING on eth0:
** Source from eth1 subnet
** MASQUERADE to eth0 IP
* Preserve rules marked "RBM-PORT-FORWARD"

. DNS Server Access:
* If
{rbrr_dns_server}
is configured:
** Add rule to RBM-EGRESS accepting traffic to
{rbrr_dns_server}
** Add rule to RBM-FORWARD accepting traffic to
{rbrr_dns_server}
* These rules are independent of
{rbrn_uplink_access_mode}
and
{rbrn_uplink_allowed_cidrs}

. Filter Configuration:
* If
{rbrn_uplink_access_mode} is global:
** Accept all output on eth0 in RBM-EGRESS
** Accept all forward from eth1 in RBM-FORWARD
* If
{rbrn_uplink_access_mode} is allowlist:
** For each CIDR in
{rbrn_uplink_allowed_cidrs}:
*** Accept in RBM-EGRESS matching destination
*** Accept in RBM-FORWARD matching destination
* Maintain existing port rules

==== Phase 4: {scr_dns_step}

If
{rbrn_uplink_dns_mode}
is disabled, the
{at_sentry_container}:

* Blocks all DNS traffic in RBM-FORWARD from eth1
* Blocks all DNS traffic in RBM-EGRESS to eth0
* Removes any existing DNS-related NAT rules

If
{rbrn_uplink_dns_mode}
is global or allowlist, the
{at_sentry_container}
executes this sequence:

. DNS Server Validation:
+
--
* Verify
{rbrr_dns_server}
if specified:
** TCP connect test to port 53 with 5 second timeout
** UDP query for "." record with 6 second timeout
* Fail if either test unsuccessful
--

. Dnsmasq Configuration:
+
--
* Core Settings:
** Interface binding limited strictly to
{at_enclave_network}
interface eth1
** Use {rbrn_enclave_sentry_ip}
for listen-address
** DNS service only - all DHCP functionality explicitly disabled
** DNS resolution configuration isolated from host system settings
** Clear existing upstream server configuration
** Configure upstream DNS:
*** Use configured upstream server when
{rbrr_dns_server}
specified
*** Otherwise use host resolver via external network interface

* Cache Settings:
** Cache size: 1000 entries
** Minimum TTL: 600 seconds
** Maximum TTL: 3600 seconds

* Query Control:
** When
{rbrn_uplink_dns_mode}
is global:
*** Accept all DNS queries from eth1
*** Forward queries unmodified to upstream
*** Log each query with timestamp and resolution
*** Return all responses unmodified

** When
{rbrn_uplink_dns_mode}
is allowlist:
*** Load allowed domains from
{rbrn_uplink_allowed_domains}
*** Accept queries only for allowed domains
*** Return NXDOMAIN for all other queries
*** Configure dnsmasq logging directly with:
**** log-queries=extra
**** log-facility=/var/log/dnsmasq.log
**** log-dhcp
**** Extra logging options in dnsmasq.conf:
***** log-debug (for detailed query info)
***** log-async=20 (for better performance)
--

. Filter Rules:
+
--
* Accept DNS traffic in RBM-FORWARD from eth1:
** UDP port 53
** TCP port 53
* Accept DNS forwarding in RBM-EGRESS to eth0:
** UDP/TCP port 53 to
{rbrr_dns_server}
** Or to host resolver if no server specified
--

. NAT Configuration:
+
--
* DNAT eth1 DNS to
{rbrn_enclave_sentry_ip}:53
* SNAT outbound queries to eth0 IP
* Preserve existing port forwarding rules
--

== {opbs_bottle_start}

This sequence of
{cmk_recipe_line_s}
within the
{at_rbm_console}
establishes a new sessile service instance:

=== {mkr_bottle_cleanup}

This sequence of
{cmk_recipe_line_s}
within the
{opbs_bottle_start}
removes any previous container instance:

* Query existence of 
{at_bottle_container}
named
{at_bottle_container_name}.
If found:
. Terminate container with 5-second timeout
. Force remove container if still present after timeout
. Verify complete removal from container runtime

=== {mkr_bottle_launch}

This sequence of
{cmk_recipe_line_s}
within the
{opbs_bottle_start}
instantiates and configures the service container:

. Launch
{at_bottle_container}:
* Image
{rbrn_bottle_moniker}:{rbrn_bottle_image_tag}
* Name
{at_bottle_container_name}
* Attach exclusively to
{at_enclave_network}
* Apply
{rbrn_volume_mounts}
specifications
* Set restart policy to "unless-stopped"

. Perform post-launch network configuration:
* Allow container initialization with podman's default network configuration
* Verify container network interface is active using {mkc_interface_check}

== {opbr_bottle_run}

This sequence of
{cmk_recipe_line_s}
within the
{at_rbm_console}
executes an agile service operation:

== VM Management Operations

This section defines the operations for managing Podman VM images within the {at_rbm_system}.

=== {ops_rbv_check}

The {ops_rbv_check} verifies whether newer VM images are available upstream by comparing SHA digests. This operation uses a temporary {at_stash_machine} with crane installed to query registries without modifying configuration.

==== {ops_rbv_check} Sequence

. Create Temporary {at_stash_machine}
* Initialize new Podman machine using name from
{rbrr_stash_machine}
* Start the machine and install crane from
{rbrr_crane_tar_gz}
* Verify crane installation completed successfully

. Query Upstream Image
* Use crane to query digest of
{rbrr_chosen_vmimage_origin}:{rbrr_chosen_podman_version}
* Store the returned SHA digest for comparison
* Fail if query returns no valid digest

. Verify Custom FQIN (if applicable)
* If
{rbrr_chosen_vmimage_fqin}
differs from standard origin:version pattern:
** Query the FQIN directly using crane
** Compare FQIN digest against upstream digest
** Report any discrepancies as warnings

. Cleanup Resources
* Stop and remove the temporary
{at_stash_machine}
* Ensure no orphaned resources remain

. Report Findings
* Display current
{rbrr_chosen_vmimage_sha}
value
* Display latest upstream SHA digest
* Indicate required actions:
** If SHA not set: request user to set
{rbrr_chosen_vmimage_sha}
** If SHA differs: notify update available
** If SHA matches: confirm system is current

=== {ops_rbv_mirror}

The
{ops_rbv_mirror}
copies the chosen VM image from upstream to the user's GHCR, ensuring consistent deployment across environments.
This operation validates preconditions, performs the copy, and verifies podman can access the mirrored image.

==== {ops_rbv_mirror} Sequence

. Validate Preconditions
* If using standard origin (FQIN starts with origin):
** Query current upstream SHA using crane
** Compare with
{rbrr_chosen_vmimage_sha}
if defined
** Fail if SHAs do not match, directing user to run
{ops_rbv_check}
first

. Generate GHCR Target
* Construct GHCR tag using registry naming pattern
* Format:
ghcr.io/{rbrr_registry_owner}/{rbrr_registry_name}/vmimage:{rbrr_chosen_podman_version}

. Check Existing Mirror
* Query GHCR for existing image at target location
* If found and SHA defined:
** Verify existing image SHA matches
{rbrr_chosen_vmimage_sha}
** Exit successfully if match confirmed
** Fail if SHAs differ

. Copy Image to GHCR
* Use crane to copy from
{rbrr_chosen_vmimage_fqin}
to GHCR target
* Verify copy completed without errors
* Confirm transferred image integrity

. Verify Copied Image
* If
{rbrr_chosen_vmimage_sha}
is defined:
** Query digest of newly copied GHCR image
** Compare with expected SHA
** Fail if verification shows mismatch

. Validate Podman Access
* Initialize test Podman machine using mirrored image
* Capture initialization output through standard scrubbing process
* Extract reported SHA from initialization
* Compare with
{rbrr_chosen_vmimage_sha}
if defined
* Remove test machine after validation

. Report Success
* Confirm mirror operation completed
* Note image available at GHCR location
* Verify podman can successfully use mirrored image

=== {mkr_bottle_create}

This sequence of
{cmk_recipe_line_s}
within the
{opbr_bottle_run}
establishes the transient container:

* Create container from
{rbrn_bottle_moniker}:{rbrn_bottle_image_tag}
* Allow podman to assign unique container name
* Attach exclusively to
{at_enclave_network}
* Configure DNS resolver to
{at_sentry_container}
address
* Apply
{rbrn_volume_mounts}
specifications
* Enable automatic cleanup on exit (--rm)

=== {mkr_command_exec}

This sequence of
{cmk_recipe_line_s}
within the
{opbr_bottle_run}
executes the requested operation:

* Execute specified command in container

== {cmk_script} Requirements

The following requirements apply to all {cmk_script_s} within the {cmk_discipline}:

=== Environment Variable Availability

Each
{at_startup_script}
inherits and validates
{cfg_regime_variable_s}
through makefile export directives:

. At script start, validate existence of all referenced variables using `: ${VARIABLE:?} && echo "VARIABLE = ${VARIABLE}"`
. {cfg_base_regime}, {cfg_station_regime}, and {cfg_nameplate_regime}
export their respective variables

=== Environment and Structure Requirements
- Each
{cmk_script}
begins with `#!/bin/sh`
- Each
{cmk_script}
enables `set -e`
- Each
{cmk_script}
validates required environment variables using `: ${VARIABLE:?}`
- Each
{cmk_script}
echoes validated variable values

=== Error Handling Requirements 
- Every command that can fail is followed by `|| exit N` where N is a positive integer
- Error handling applies within if/then/else blocks and loops
- Common commands like echo use exit code 99 for failures
* Exit Code Ranges and Meanings:
** 10: IPTables failures
** 20: Port forwarding configuration failures
** 30: Network access configuration failures 
** 31: Network routing/filtering setup failures
** 32: CIDR rule configuration failures
** 40: DNS server connectivity check failures
** 41: DNS configuration file writing failures
** 42: DNS service startup failures
** 43: DNS rule configuration failures

=== Control Flow Requirements
- if/then/else statements are permitted for clear flow control
- for/while loops are permitted for iteration
- All commands within control structures maintain error handling
- Break and continue are permitted within loops
- No command substitution without error handling
- No function definitions permitted
- All logic is expressed through direct command sequences

=== Operation Logging Requirements
- Each echo uses double quotes
- Each log line begins with "RBSpN: " where N is the phase number (1-5)
- Echo statements replace inline comments
- No shell comments (# or multi-line) permitted
- Debug echo statements precede significant operations

=== Example Conformant Script Structure

```sh
#!/bin/sh
set -e

: "${INPUT_DIR:?}"
: "${OUTPUT_DIR:?}"

echo "XYZp1: Processing files from ${INPUT_DIR} to ${OUTPUT_DIR}"

if [ ! -d "${INPUT_DIR}" ]; then
    echo "XYZp1: Input directory not found"
    exit 1
fi

if [ ! -d "${OUTPUT_DIR}" ]; then
    echo "XYZp1: Creating output directory"
    mkdir -p "${OUTPUT_DIR}" || exit 2
fi

echo "XYZp1: Beginning file conversion"
for src in "${INPUT_DIR}"/*.txt; do
    if [ -f "${src}" ]; then
        echo "XYZp1: Processing ${src}"
        dest="${OUTPUT_DIR}/$(basename "${src}" .txt).upper"
        tr '[:lower:]' '[:upper:]' < "${src}" > "${dest}" || exit 3
        wc -l "${dest}" > "${dest}.count" || exit 4
    fi
done

echo "XYZp1: Processing complete"
```

== {cfg_base_regime} Definitions

This section defines the
{cfg_regime_variable_s}
which come from the
{cfg_base_regime}
that defines the core configuration for the
{at_rbm_system}.

[[rbrr_dns_server]]
{rbrr_dns_server}:: 
An IPv4 address (e.g., "8.8.8.8") that specifies the upstream DNS resolver for all 
{at_bottle_service_s}.
When configured, the
{at_sentry_container}
forwards DNS queries to this address instead of the host resolver, enabling consistent name resolution across all
{at_bottle_container_s}
while maintaining the established security boundaries.
If not specified, the system uses the host's resolver configuration.

[[rbrr_nameplate_path]]
{rbrr_nameplate_path}:: 
A relative path (e.g., "recipes/nameplates") that specifies the directory containing all
{at_nameplate}
definitions for this
{at_rbm_system}.
This path is resolved relative to the
{at_rbm_repo}
root, enabling consistent access to configuration files across different installations.
If not specified, the system uses "nameplates" in the repository root.

[[rbrr_registry_server]]
{rbrr_registry_server}:: 
A fully-qualified domain name and optional port (e.g., "registry.example.com:5000") that specifies the container registry server for all
{at_bottle_service_s}.
The
{at_rbm_system}
retrieves all
{st_published_image_s}
from this registry, ensuring consistent image sourcing across the installation.
If not specified, the system uses the default public container registry.

[[rbrr_registry_owner]]
{rbrr_registry_owner}::
A GitHub username or organization name that specifies the owner of the container registry namespace where
{st_published_image_s}
are stored.

[[rbrr_registry_name]]
{rbrr_registry_name}::
A GitHub repository name that defines the repository within the container registry and forms the complete registry path when combined with
{rbrr_registry_owner}.

[[rbrr_github_pat_env]]
{rbrr_github_pat_env}::
A file path to an environment file containing GitHub credentials for API and
{at_container_registry}
authentication with required RBV_USERNAME and RBV_PAT variables.

[[rbrr_build_architectures]]
{rbrr_build_architectures}::
A space-separated list of platform identifiers in os/arch format that specifies target architectures for
{st_container}
builds.

[[rbrr_history_dir]]
{rbrr_history_dir}::
A relative path from the
{at_rbm_system}
root that specifies the local directory for storing build artifacts and history from GitHub Actions workflows.

[[rbrr_chosen_podman_version]]
{rbrr_chosen_podman_version}::
The user-selected Podman version (e.g., "5.5") that determines which VM image version to use for the RBM system.

[[rbrr_chosen_vmimage_origin]]
{rbrr_chosen_vmimage_origin}::
The base container registry path (e.g., "quay.io/podman/machine-os-wsl") from which Podman VM images are sourced.

[[rbrr_chosen_vmimage_fqin]]
{rbrr_chosen_vmimage_fqin}::
The fully qualified image name including registry, repository, and tag that specifies the exact VM image to use.

[[rbrr_chosen_vmimage_sha]]
{rbrr_chosen_vmimage_sha}::
The SHA256 digest of the VM image that ensures consistency and integrity across deployments.

[[rbrr_chosen_identity]]
{rbrr_chosen_identity}::
A marker indicating the configuration state or version of the RBM system deployment.

[[rbrr_stash_machine]]
{rbrr_stash_machine}::
The name identifier for temporary Podman machines created for registry operations and image verification.

[[rbrr_operational_machine]]
{rbrr_operational_machine}::
The name identifier for the primary Podman machine that runs bottle services in production.

[[rbrr_crane_tar_gz]]
{rbrr_crane_tar_gz}::
The URL for downloading the crane utility used for container registry operations and image copying.

== Architecture Term Definitions

[[bottle_image]]
{at_bottle_image}::
A
{st_image}
that contains the service functionality intended to run within a
{at_bottle_container}.
This image is stored in the
{at_container_registry}
and is referenced by
{rbrn_bottle_repo_path} and {rbrn_bottle_image_tag}.

[[bottle_service]]
{at_bottle_service}::
A complete service instance consisting of one
{at_sentry_container},
one
{at_censer_container},
and one
{at_bottle_container}
working together to provide secure functionality. Each service maintains strict file system and network boundaries while enabling controlled access to resources.

[[sessile_service]]
{at_sessile_service}::
A
{at_bottle_service}
that maintains a persistent
{at_bottle_container}
running continuously to provide long-term services like development tools or network services.
These services maintain state and provide stable network ports while enforcing all
{at_sentry_container}
security constraints.

[[agile_service]]
{at_agile_service}::
A
{at_bottle_service}
that creates temporary
{at_bottle_container_s}
for single operations, immediately removing them upon completion. This pattern suits toolchain operations that primarily operate through
{st_volume_mount_s}
with minimal network requirements.

[[workstation]]
{at_workstation}::
The local computing environment where the
{rbtr_consumer}
runs the
{at_rbm_system}.
This environment hosts all
{at_bottle_service_s}
and their associated networks while providing isolation from other system resources.

[[enclave_network]]
{at_enclave_network}::
An isolated network connecting a
{at_bottle_container}
to its
{at_sentry_container},
using the
// TODO involve the new network namespace
{rbrn_enclave_base_ip}/{rbrn_enclave_netmask}
for addressing.
This network enforces strict security policies and provides the only network communication path for the
{at_bottle_container}.

[[transit_network]]
{at_transit_network}::
The network interface that connects a
{at_sentry_container}
to the
{at_workstation_p}
network resources.
This network enables controlled external communication while maintaining security boundaries.

[[moniker]]
{at_moniker}::
A unique identifier specified by
{rbrn_moniker}
that distinguishes each
{at_bottle_service}
instance within the
{at_rbm_system}.
This identifier forms the basis for container names and network identifiers.

[[nameplate]]
{at_nameplate}::
A configuration file stored in
{rbrr_nameplate_path}
that defines all aspects of a
{at_bottle_service},
including its images, network settings, and security policies.
Each nameplate provides a complete service specification that the
{at_rbm_system}
can instantiate.

[[transit_network_name]]
{at_transit_network_name}::
The unique network name formed by appending `-uplink` to the service's
{rbrn_moniker}.
The
{at_rbm_system}
uses this consistent naming pattern
`${rbrn_moniker}-uplink`
to identify the external-facing network for each
{at_sentry_container}
within the container runtime environment.

[[enclave_network_name]]
{at_enclave_network_name}::
The unique network name formed by appending `-enclave` to the service's
{rbrn_moniker}.
The
{at_rbm_system}
uses this consistent naming pattern `${rbrn_moniker}-enclave` to identify the isolated internal network connecting each
{at_bottle_container}
to its
{at_sentry_container}.

[[enclave_namespace_name]]
{at_enclave_namespace_name}::
The
// TODO enclave, transit, network, but then namespace: this all needs a term rebalance... but later
{at_bottle_service}
unique namespace name formed by appending `-namespace` to the service's
{rbrn_moniker}.
// TODO this definition is not precise, but I'm not currently smart enough to repair it
The
{at_rbm_system}
uses this consistent naming pattern `${rbrn_moniker}-namespace` to identify the isolated internal network namespace connecting each
{at_bottle_container}
to its
{at_sentry_container}.

[[sentry_image]]
{at_sentry_image}::
A
{st_image}
that contains the security enforcement functionality for a
{at_sentry_container}.
This image is stored in the
{at_container_registry}
and is referenced by
{rbrn_sentry_repo_path} and {rbrn_sentry_image_tag}.

[[sentry_container]]
{at_sentry_container}::
A privileged container that enforces network security policies for a
{at_bottle_service}.
It connects to both
{at_transit_network} and {at_enclave_network},
controlling all network traffic and providing DNS services.

[[censer_container]]
{at_censer_container}::
A privileged container using a minimal image that establishes and configures the network namespace before the
{at_bottle_container}
starts.
It sets up routing to ensure all traffic flows through the
{at_sentry_container},
then shares this pre-configured namespace with the
{at_bottle_container}
via `--net=container:censer`.
This ensures security policies are enforced from the first packet.

[[bottle_container]]
{at_bottle_container}::
A container that runs the actual service functionality, inheriting its network namespace from the
{at_censer_container}.
It is connected only to the
{at_enclave_network}
and configured with specific
{st_volume_mount_s}.
All external communication flows through its associated
{at_sentry_container}.

[[bottle_container_name]]
{at_bottle_container_name}::
The 
{at_bottle_container}
formed by appending `-bottle` to the service's
{rbrn_moniker}.
The
{at_rbm_system}
uses this consistent naming pattern
`${rbrn_moniker}-bottle`
to identify all
{at_bottle_container_s}
within the container runtime environment, enabling reliable service management and status tracking.

[[sentry_container_name]]
{at_sentry_container_name}::
The unique container name formed by appending `-sentry` to the service's
{rbrn_moniker}.
The
{at_rbm_system}
uses this consistent naming pattern
`${rbrn_moniker}-sentry`
to identify all
{at_sentry_container_s}
within the container runtime environment, enabling reliable security policy enforcement and network management.

[[rbm_console]]
{at_rbm_console}::
The primary
{cmk_makefile}
that implements the
{at_rbm_system_p}
operational rules. It provides
{cmk_external_rule_s} and {cmk_internal_rule_s}
for managing
{at_bottle_service_s}
throughout their lifecycle.

[[rbm_config_makefile]]
{at_rbm_config_makefile}::
A
{cmk_makefile}
that manages configuration settings for the
{at_rbm_system},
including
{at_rbm_secret_s}
and environment-specific parameters.
This file typically remains outside version control to protect sensitive data.

[[rbm_system]]
{at_rbm_system}::
The complete Recipe Bottle Makefile system, consisting of the
{at_rbm_console}, {at_rbm_config_makefile}, {at_nameplate_s},
and associated tools.
This system enables secure deployment and management of containerized services.

[[rbm_secret]]
{at_rbm_secret}::
A sensitive configuration value needed by the
{at_rbm_system},
such as
registry credentials.
These secrets are managed through the
{at_rbm_config_makefile}
and kept separate from version control.

[[rbm_repo]]
{at_rbm_repo}::
The version control repository containing the
{at_rbm_system}
implementation, including the
{at_rbm_console}, {at_nameplate_s},
and documentation.
This repository serves as the authoritative source for system configuration.

[[user_repo]]
{at_user_repo}::
A version control repository owned by a
{rbtr_consumer}.
The
{rbtr_consumer}
chooses how to instantiate the
{at_rbm_system}
there, though a git subtree is a good choice.
The
{rbtr_consumer}
configures the
{at_rbm_system}
by committing a
{cfg_base_regime}
makefile and then creates a set of
{at_nameplate_s}
compliant with the
{cfg_nameplate_regime}
to define
{at_bottle_service_s}.

[[container_registry]]
{at_container_registry}:: 
A service that stores and provides access to 
{st_published_image_s}.

[[startup_script]]
{at_startup_script}::
A
{cmk_script}
that performs initialization and configuration tasks when starting a
{at_bottle_service}.
These scripts follow strict requirements for execution environment, logging, and error handling.

[[consumer]]
{rbtr_consumer}::
An individual operating the
{at_rbm_system}
to run containerized services securely on their
{at_workstation}.
{rbtr_consumer_s}
control service configuration through
{at_nameplate_s}
and interact with services through controlled interfaces, distinct from infrastructure operators who build and deploy the system.

[[at_ebpf_program]]
{at_ebpf_program}::
An eBPF TC (Traffic Control) filter program attached to the
{at_bottle_container_p}
veth interface that intercepts and rewrites Ethernet frames at the packet level.
This program replaces the default gateway IP address assigned by netavark with the
{at_sentry_container_p}
IP address in all outbound frames, ensuring all
{at_bottle_container}
network traffic routes through its associated
{at_sentry_container}
without requiring network namespace manipulation or special container privileges.

[[base_regime]]
{cfg_base_regime}::
A
{crg_regime}
that defines and enforces core system configuration through environment variables with the RBB_*
{crg_prefix}.
The
{at_rbm_repo}
contains exactly one Base Config makefile that exports these variables, establishing foundational configuration for a single
{at_rbm_system}
installation.

[[station_regime]]
{cfg_station_regime}::
A
{crg_regime}
that defines and enforces deployment-specific configuration through environment variables with the RBS_*
{crg_prefix}.
Each
{rbtr_consumer}
maintains their own Station Config makefile outside version control, containing sensitive data like authentication tokens required for their specific installation.

[[nameplate_regime]]
{cfg_nameplate_regime}::
A
{crg_regime}
that defines and enforces service configuration through environment variables with the RBRN_*
{crg_prefix}.
Each
{at_bottle_service}
has exactly one
{at_nameplate}
makefile that exports these variables, establishing that service's precise behavior and constraints.

[[regime_variable]]
{cfg_regime_variable}::
A
{crg_variable}
exported by one of the three configuration regime makefiles, following that regime's
{crg_prefix}
(RBB_*, RBS_*, or RBRN_*).
Each
{at_startup_script}
validates required
{cfg_regime_variable_s}
using the ': ${VARIABLE:?}' pattern before use.

[[stash_machine]]
{at_stash_machine}::
A temporary Podman virtual machine created with crane installed for performing registry queries and image operations.
These machines are removed after completing their specific tasks.

[[operational_machine]]
{at_operational_machine}::
The persistent Podman virtual machine that hosts all bottle services and their associated containers in the RBM system.

== {cfg_station_regime} Definitions

This section defines the
{cfg_regime_variable_s}
which come from the
{cfg_station_regime}
that defines user specific configuration for the
{at_rbm_system}.

This
{crg_regime}
provides settings that are specific to a particular local {at_rbm_repo} clone or checkout.
These settings typically contain sensitive data like authentication tokens that should not be committed to version control.

== Nameplate Configuration Regime Definitions

This section defines the
{cfg_regime_variable_s}
which come from the
{cfg_nameplate_regime}.
Each
{at_bottle_service}
has exactly one
{at_nameplate}
makefile that exports these variables, establishing that service's precise behavior and constraints.

[[rbrn_moniker]]
{rbrn_moniker}::
A unique identifier that distinguishes this
{at_bottle_service}
from others in the
{at_rbm_system}.

[[rbrn_description]]
{rbrn_description}::
A human-readable explanation of the
{at_bottle_service_p}
purpose and function.

[[rbrn_sentry_moniker]]
{rbrn_sentry_moniker}::
The image moniker identifier for the
{at_sentry_image}
in the
{at_container_registry}.

[[rbrn_bottle_moniker]]
{rbrn_bottle_moniker}::
The image moniker identifier for the
{at_bottle_image}
in the
{at_container_registry}.

[[rbrn_sentry_image_tag]]
{rbrn_sentry_image_tag}::
The version identifier for the
{at_sentry_image}
to be deployed.

[[rbrn_bottle_image_tag]]
{rbrn_bottle_image_tag}::
The version identifier for the
{at_bottle_image}
to be deployed.

[[rbrn_entry_mode]]
{rbrn_entry_mode}::
The mode controlling whether the
{at_bottle_service}
exposes network ports through the
{at_sentry_container}.
When disabled, no entry ports are configured.
When enabled, entry ports are mapped through the
{at_sentry_container}.

[[rbrn_entry_port_workstation]]
{rbrn_entry_port_workstation}::
The network port number exposed on the
{at_transit_network}
when port service is enabled.

[[rbrn_entry_port_enclave]]
{rbrn_entry_port_enclave}::
The network port number used for communication between
{at_sentry_container}
and
{at_bottle_container}
across the
{at_enclave_network}.

[[rbrn_uplink_dns_mode]]
{rbrn_uplink_dns_mode}::
The DNS resolution mode for bottle-initiated requests through the
{at_sentry_container}.
When disabled, the
{at_sentry_container} blocks all DNS queries from the
{at_bottle_container}.
When global, the
{at_sentry_container}
forwards all DNS queries unmodified to upstream.
When allowlist, the
{at_sentry_container}
allows resolution only for domains listed in
{rbrn_uplink_allowed_domains}
and returns NXDOMAIN for all other queries.

[[rbrn_uplink_access_mode]]
{rbrn_uplink_access_mode}::
The IP access mode for bottle-initiated connections through the
{at_transit_network}.
When disabled, all non-port traffic is blocked.
When global, unrestricted IP access is allowed.
When allowlist, only traffic to CIDRs listed in
{rbrn_uplink_allowed_cidrs}
is permitted.

[[rbrn_uplink_allowed_cidrs]]
{rbrn_uplink_allowed_cidrs}::
A space-separated list of CIDR ranges that define allowed outbound traffic through the
{at_transit_network}.

[[rbrn_uplink_allowed_domains]]
{rbrn_uplink_allowed_domains}::
A space-separated list of domain names that are allowed for DNS resolution through the
{at_sentry_container}.

[[rbrn_enclave_netmask]]
{rbrn_enclave_netmask}::
The network mask width defining the size of address space available for the
{at_enclave_network}.

[[rbrn_enclave_base_ip]]
{rbrn_enclave_base_ip}::
The base IPv4 address that establishes the starting point of the
{at_enclave_network}
range.

[[rbrn_enclave_sentry_ip]]
{rbrn_enclave_sentry_ip}::
The permanent gateway IP address (typically .0.1) assigned to the
{at_sentry_container}
on the
{at_enclave_network}
after removing the initial auto-assigned address. This address serves as the gateway for all
{at_bottle_container_s}
on the
{at_enclave_network}.

[[rbrn_volume_mounts]]
{rbrn_volume_mounts}::
A space-separated list of Podman volume mount arguments that define filesystem access for the
{at_bottle_container}.

== Console Makefile Elements

[[cmk_console_discipline]]
{cmk_discipline}::
A systematic approach to automation that defines patterns for creating reliable, maintainable build systems through careful rule organization, strict error handling, and standardized operational sequencing. This discipline establishes conventions for multi-pass construction, naming patterns, and the relationships between makefiles, scripts, and configuration files.

[[cmk_makefile]]
{cmk_makefile}::
A specific makefile implementation that follows the
{cmk_discipline},
expressing concrete automation through properly structured
{cmk_external_rule_s}, {cmk_internal_rule_s}, and {cmk_script_s}
while maintaining prescribed error handling and operational patterns.

[[cmk_clause]]
{cmk_clause}::
A standardized sequence of commands or expressions within a
{cmk_makefile}
that provides consistent behavior across different rules and contexts while maintaining
{cmk_discipline}
patterns.
These clauses can be invoked directly, used with $(call), or incorporated into larger recipes.

[[cmk_recipe_line]]
{cmk_recipe_line}::
A single command line within a
{cmk_external_rule} or {cmk_internal_rule}
that follows
{cmk_discipline}
conventions, expressing one concrete bash action.

[[cmk_probe_line]]
{cmk_probe_line}::
A single makefile recipe line that executes a precise command within a container context.
Recipe lines form the verification backbone of the
{cmk_discipline},
ensuring proper state progression through operational sequences.

[[cmk_internal_rule]]
{cmk_internal_rule}::
A makefile rule that coordinates implementation details within the
{cmk_discipline},
called by other
{cmk_external_rule_s} or {cmk_internal_rule_s}
to perform specific operational sequences.

[[cmk_external_rule]]
{cmk_external_rule}::
A makefile rule that provides primary interface points for
{rbtr_consumer}
use, typically invoked by shell scripts or direct user commands to initiate major system operations.

[[cmk_script]]
{cmk_script}::
A bash script that exists as an integral part of a
{cmk_discipline}
installation, executed under control of
{cmk_external_rule_s} or {cmk_internal_rule_s}
and sharing the makefile's operational context and constraints.

[[cmk_script_line]]
{cmk_script_line}::
One or more command lines within a
{cmk_script}
that achieve a particular action in the script's operational sequence, following
{cmk_discipline}
conventions for error handling and status reporting.
Any failure in a script line results in a non-zero exit status.

== Supporting Infrastructure Definitions

[[container]]
{st_container}:: 
A lightweight, standalone executable package containing all necessary components to run a service.

[[dockerfile]]
{st_dockerfile}:: 
A specification file that defines how to build a 
{st_image}.

[[image]] 
{st_image}:: 
A template used to create 
{st_container}
instances, containing the service's code, runtime, and dependencies.

[[image_store]]
{st_image_store}:: 
The local storage location for 
{st_image_s}
on the 
{rbtr_consumer_p}
{at_workstation}.

[[published_image]]
{st_published_image}:: 
A 
{st_image}
that has been uploaded to a 
{at_container_registry}.

[[port_map]]
{st_port_map}:: 
A specification of how network ports should be exposed and mapped between 
{st_container_s}
and the host system.

[[volume_mount]]
{st_volume_mount}:: 
A configuration that allows 
{st_container_s}
to access specified portions of the host filesystem.

[[subnet]]
{st_subnet}:: 
A logically visible subdivision of IP network address space.

[[gateway]]
{st_gateway}:: 
A network node that serves as the entry point between two networks.

== Startup Operation Definitions

[[opss_sentry_start]]
{opss_sentry_start}:: 
The complete ordered set of operations required to establish a new {at_bottle_service}, consisting of distinct phases that must be validated before progression.

[[scr_security_config]]
{scr_security_config}:: 
{cmk_script}
executed by the
{cmk_makefile}
upon the
{at_sentry_container}
that establishes security controls through
{scr_iptables_init}, {scr_access_setup}, {scr_port_setup}, and the {scr_dns_step}.

[[mkr_network_create]]
{mkr_network_create}:: 
Operation that establishes both the
{at_transit_network} and {at_enclave_network}
using podman network primitives.

[[mkr_sentry_run]]
{mkr_sentry_run}:: 
Operation that launches the
{at_sentry_container}
with required privileges and initial network configuration.

[[mkr_network_connect]]
{mkr_network_connect}:: 
Operation that establishes the connection between the
{at_sentry_container} and {at_enclave_network}.

[[mkr_bottle_cleanup]]
{mkr_bottle_cleanup}:: 
A sequence of
{cmk_recipe_line_s}
that ensures complete removal of any existing
{at_bottle_container}
matching the service name.
The sequence guarantees clean system state through progressively stronger actions.

[[mkr_bottle_launch]]
{mkr_bottle_launch}:: 
A sequence of
{cmk_recipe_line_s}
that establishes a persistent
{at_bottle_container}
with appropriate service configuration and restart behavior.

[[mkr_bottle_create]]
{mkr_bottle_create}:: 
A sequence of
{cmk_recipe_line_s}
that establishes a transient
{at_bottle_container}
configured for single-operation execution and automatic cleanup.

[[mkr_command_exec]]
{mkr_command_exec}:: 
A sequence of
{cmk_recipe_line_s}
that executes the requested operation within a transient
{at_bottle_container}
while preserving command status.

[[mkc_interface_check]]
{mkc_interface_check}::
A
{cmk_clause}
that verifies network interface presence and configuration within a container context:

[source,makefile]
----
# Waits until interface has an IPv4 address assigned, fails after timeout
# @param 1 interface   Network interface name to check
# @param 2 timeout_s   Number of seconds to wait before failing
define check_interface
timeout $(2)s sh -c "while ! ip addr show $(1) | grep -q \"inet \"; do sleep 0.2; done"
endef
----

This clause generates precise validation commands for network interface readiness probes, accepting an interface name and timeout duration as parameters.

[[ops_rbv_check]]
{ops_rbv_check}::
An operation that queries upstream registries to compare the latest available VM image SHA against the currently configured SHA.
Reports discrepancies without modifying configuration files.

[[ops_rbv_mirror]]
{ops_rbv_mirror}::
An operation that copies the chosen VM image from upstream to the user's GHCR, verifying SHA consistency throughout.
Ensures podman machine init retrieves the same image whether from upstream or the mirror.

[[scr_iptables_init]]
{scr_iptables_init}:: 
Operation that establishes the basic IPTables rules and chains within a {at_sentry_container}.

[[scr_access_setup]]
{scr_access_setup}:: 
Operation configuring network access controls including NAT rules and connection tracking.

[[scr_port_setup]]
{scr_port_setup}:: 
Operation establishing port mapping and service exposure rules within a
{at_sentry_container}.

[[scr_dns_step]]
{scr_dns_step}:: 
Operation configuring DNS resolution controls and domain access restrictions.

[[opbs_bottle_start]]
{opbs_bottle_start}:: 
An
{cmk_external_rule}
that establishes a new
{at_sessile_service}
through controlled container instantiation and configuration.
The rule coordinates container lifecycle, network attachment, and volume management to ensure proper service isolation and accessibility.

[[opbr_bottle_run]]
{opbr_bottle_run}:: 
An
{cmk_external_rule}
that creates and executes a new
{at_agile_service}
instance through controlled container instantiation and command execution.
The rule coordinates container creation, network attachment, and immediate cleanup to ensure proper service isolation during the command lifecycle.

